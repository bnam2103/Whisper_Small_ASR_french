{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.0)\n",
      "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.8.0)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.35.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.14.1)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0->torchaudio) (77.0.1)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.62.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scipy>=1.6.0 (from librosa)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.10)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.45.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.1.31)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0->torchaudio) (2.1.5)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\n",
      "Downloading numba-0.62.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m158.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m266.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m219.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "Downloading llvmlite-0.45.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m174.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, soxr, scipy, msgpack, llvmlite, lazy_loader, joblib, audioread, soundfile, scikit-learn, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 joblib-1.5.2 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.45.0 msgpack-1.1.1 numba-0.62.0 pooch-1.8.2 scikit-learn-1.7.2 scipy-1.16.2 soundfile-0.13.1 soxr-1.0.0 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets==3.6.0 evaluate jiwer torchaudio librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# from datasets import Dataset\n",
    "# df = pd.read_csv(\"test.tsv\", sep='\\t')\n",
    "# df = df[['path','sentence']]\n",
    "\n",
    "# # Define the folder containing your audio files\n",
    "# audio_folder = \"test_audio\"\n",
    "\n",
    "# # List all files in the folder\n",
    "# audio_files = set(os.listdir(audio_folder))\n",
    "\n",
    "# # Filter the DataFrame to include only rows where the 'path' exists in the folder\n",
    "# df_filtered = df[df['path'].isin(audio_files)]\n",
    "\n",
    "# # Optionally, save the filtered DataFrame\n",
    "# df_filtered.to_csv(\"filtered_dataframe.csv\", index=False)\n",
    "# df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_pandas(df_filtered)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aa92786204418caa7a22fbbba92687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "cv_17 = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"fr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'sentence'],\n",
       "    num_rows: 16159\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = cv_17.remove_columns(['path','client_id','up_votes','down_votes','age','gender','accent','locale','segment','variant'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9441cdf58e014f0596e2a6e1c7d6671d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate: 48000\n",
      "array:[ 0.00000000e+00 -4.95903318e-12 -4.28973263e-13 ... -5.87302275e-05\n",
      " -8.39358181e-05 -7.38822855e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'sentence'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = dataset.select(range(1))\n",
    "def print_first_audio(batch): \n",
    "    first_audio = batch[\"audio\"][0] \n",
    "    print(f\"Sampling rate: {first_audio['sampling_rate']}\")\n",
    "    print(f\"array:{first_audio['array']}\")\n",
    "    return batch\n",
    "filtered_data.map(print_first_audio, batched=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'sentence'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = dataset.select(range(1))\n",
    "def print_first_audio(batch): \n",
    "    first_audio = batch[\"audio\"][0] \n",
    "    print(f\"Sampling rate: {first_audio['sampling_rate']}\")\n",
    "    print(f\"array:{first_audio['array']}\")\n",
    "    return batch\n",
    "filtered_data.map(print_first_audio, batched=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data = dataset.select(range(5))\n",
    "\n",
    "# from datasets import Audio\n",
    "\n",
    "# filtered_data = filtered_data.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Batched function to transcribe a batch of examples\n",
    "def transcribe_batch(batch):\n",
    "    # Extract audio arrays and sampling rates\n",
    "    audio_arrays = batch[\"audio\"]\n",
    "    inputs = [a[\"array\"] for a in audio_arrays]\n",
    "    sentences = batch[\"sentence\"]\n",
    "    # Transcribe using the pipeline\n",
    "    results = pipe(inputs)\n",
    "\n",
    "    # Extract just the text\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "\n",
    "    # Return the transcriptions as a new column\n",
    "    return {\n",
    "        \"transcription\": transcriptions\n",
    "    }\n",
    "\n",
    "# Apply the batched function to the dataset\n",
    "transcriptions = dataset.map(transcribe_batch, batched=True, batch_size=8)  # You can tune batch_size\n",
    "\n",
    "# Now transcribed_dataset contains: 'audio', 'sentence', 'transcription'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'sentence', 'transcription'],\n",
       "    num_rows: 16159\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: Ce dernier a évolué tout au long de l'histoire romaine.\n",
      "Transcription:  Ce dernier est volé tout au long de l'histoire romaine.\n",
      "--------------------------------------------------\n",
      "Ground Truth: Son actionnaire majoritaire est le Conseil territorial de Saint-Pierre-et-Miquelon.\n",
      "Transcription:  son actionnaire majoritaire et le conseil territorial de Saint-Pierre et Miqueulon\n",
      "--------------------------------------------------\n",
      "Ground Truth: Ce site contient quatre tombeaux de la dynastie achéménide et sept des Sassanides.\n",
      "Transcription:  Ce site contient 4 tombeaux de la dynastie Hemenid et 7 des Sassanid.\n",
      "--------------------------------------------------\n",
      "Ground Truth: J'ai dit que les acteurs de bois avaient, selon moi, beaucoup d'avantages sur les autres.\n",
      "Transcription:  J'ai dit que les acteurs de bois avaient, selon moi, beaucoup davantage sur les autres.\n",
      "--------------------------------------------------\n",
      "Ground Truth: Les Pays-Bas ont remporté toutes les éditions.\n",
      "Transcription:  Le Pays-Bas reportait toutes les éditions.\n",
      "--------------------------------------------------\n",
      "\n",
      "Average WER: 0.34046263655426456\n",
      "Average CER: 0.1680234764998654\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "total_wer = 0\n",
    "total_cer = 0\n",
    "\n",
    "# Store the WER and CER values for averaging later\n",
    "wer_values = []\n",
    "cer_values = []\n",
    "\n",
    "# Now calculate WER and CER for each transcription and store them\n",
    "for transcription in transcriptions:\n",
    "    ground_truth = transcription['sentence']\n",
    "    predicted_text = transcription['transcription']\n",
    "    \n",
    "    # Calculate WER and CER for each transcription\n",
    "    word_error_rate = wer(ground_truth, predicted_text)  # Calculate WER\n",
    "    char_error_rate = cer(ground_truth, predicted_text)  # Calculate CER\n",
    "    \n",
    "    # Append the values to lists for later averaging\n",
    "    wer_values.append(word_error_rate)\n",
    "    cer_values.append(char_error_rate)\n",
    "    \n",
    "    # Print the top 5 transcriptions based on WER (or you can choose CER as well)\n",
    "    if len(wer_values) <= 5:\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Transcription: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Calculate the average WER and CER\n",
    "avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0\n",
    "avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0\n",
    "\n",
    "# Print the average WER and CER\n",
    "print(\"\\nAverage WER:\", avg_wer)\n",
    "print(\"Average CER:\", avg_cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838f8bb516f64b76b7042ad361ad60ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faad8af970344bff92408d8b8bf5624f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'mozilla-foundation/common_voice_16_0' is a gated dataset on the Hub. Visit the dataset page at https://huggingface.co/datasets/mozilla-foundation/common_voice_16_0 to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[32m      3\u001b[39m notebook_login()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cv_16 = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmozilla-foundation/common_voice_16_0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:1782\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m     download_config = download_config.copy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[32m   1781\u001b[39m     download_config.storage_options.update(storage_options)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[32m   1795\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:1652\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:1636\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[39m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m e.response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m   1635\u001b[39m         message += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to ask for access.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1636\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m   1639\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1640\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'mozilla-foundation/common_voice_16_0' is a gated dataset on the Hub. Visit the dataset page at https://huggingface.co/datasets/mozilla-foundation/common_voice_16_0 to ask for access."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "cv_16 = load_dataset(\"mozilla-foundation/common_voice_16_0\", \"fr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cv_16.remove_columns(['path','client_id','up_votes','down_votes','age','gender','accent','locale','segment','variant'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Batched function to transcribe a batch of examples\n",
    "def transcribe_batch(batch):\n",
    "    # Extract audio arrays and sampling rates\n",
    "    audio_arrays = batch[\"audio\"]\n",
    "    inputs = [a[\"array\"] for a in audio_arrays]\n",
    "    sentences = batch[\"sentence\"]\n",
    "    # Transcribe using the pipeline\n",
    "    results = pipe(inputs)\n",
    "\n",
    "    # Extract just the text\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "\n",
    "    # Return the transcriptions as a new column\n",
    "    return {\n",
    "        \"transcription\": transcriptions\n",
    "    }\n",
    "\n",
    "# Apply the batched function to the dataset\n",
    "transcriptions = dataset.map(transcribe_batch, batched=True, batch_size=8)  # You can tune batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "total_wer = 0\n",
    "total_cer = 0\n",
    "\n",
    "# Store the WER and CER values for averaging later\n",
    "wer_values = []\n",
    "cer_values = []\n",
    "\n",
    "# Now calculate WER and CER for each transcription and store them\n",
    "for transcription in transcriptions:\n",
    "    ground_truth = transcription['sentence']\n",
    "    predicted_text = transcription['transcription']\n",
    "    \n",
    "    # Calculate WER and CER for each transcription\n",
    "    word_error_rate = wer(ground_truth, predicted_text)  # Calculate WER\n",
    "    char_error_rate = cer(ground_truth, predicted_text)  # Calculate CER\n",
    "    \n",
    "    # Append the values to lists for later averaging\n",
    "    wer_values.append(word_error_rate)\n",
    "    cer_values.append(char_error_rate)\n",
    "    \n",
    "    # Print the top 5 transcriptions based on WER (or you can choose CER as well)\n",
    "    if len(wer_values) <= 5:\n",
    "        print(f\"Audio Path: {transcription['audio_path']}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Transcription: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Calculate the average WER and CER\n",
    "avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0\n",
    "avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0\n",
    "\n",
    "# Print the average WER and CER\n",
    "print(\"\\nAverage WER:\", avg_wer)\n",
    "print(\"Average CER:\", avg_cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "cv_15 = load_dataset(\"mozilla-foundation/common_voice_15_0\", \"fr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cv_15.remove_columns(['path','client_id','up_votes','down_votes','age','gender','accent','locale','segment','variant'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Batched function to transcribe a batch of examples\n",
    "def transcribe_batch(batch):\n",
    "    # Extract audio arrays and sampling rates\n",
    "    audio_arrays = batch[\"audio\"]\n",
    "    inputs = [a[\"array\"] for a in audio_arrays]\n",
    "    sentences = batch[\"sentence\"]\n",
    "    # Transcribe using the pipeline\n",
    "    results = pipe(inputs)\n",
    "\n",
    "    # Extract just the text\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "\n",
    "    # Return the transcriptions as a new column\n",
    "    return {\n",
    "        \"transcription\": transcriptions\n",
    "    }\n",
    "\n",
    "# Apply the batched function to the dataset\n",
    "transcriptions = dataset.map(transcribe_batch, batched=True, batch_size=8)  # You can tune batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "total_wer = 0\n",
    "total_cer = 0\n",
    "\n",
    "# Store the WER and CER values for averaging later\n",
    "wer_values = []\n",
    "cer_values = []\n",
    "\n",
    "# Now calculate WER and CER for each transcription and store them\n",
    "for transcription in transcriptions:\n",
    "    ground_truth = transcription['sentence']\n",
    "    predicted_text = transcription['transcription']\n",
    "    \n",
    "    # Calculate WER and CER for each transcription\n",
    "    word_error_rate = wer(ground_truth, predicted_text)  # Calculate WER\n",
    "    char_error_rate = cer(ground_truth, predicted_text)  # Calculate CER\n",
    "    \n",
    "    # Append the values to lists for later averaging\n",
    "    wer_values.append(word_error_rate)\n",
    "    cer_values.append(char_error_rate)\n",
    "    \n",
    "    # Print the top 5 transcriptions based on WER (or you can choose CER as well)\n",
    "    if len(wer_values) <= 5:\n",
    "        print(f\"Audio Path: {transcription['audio_path']}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Transcription: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Calculate the average WER and CER\n",
    "avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0\n",
    "avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0\n",
    "\n",
    "# Print the average WER and CER\n",
    "print(\"\\nAverage WER:\", avg_wer)\n",
    "print(\"Average CER:\", avg_cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Try loading the dataset without streaming\n",
    "dataset = load_dataset(\"facebook/multilingual_librispeech\", \"french\",split=\"test\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"test\"].remove_columns(['original_path', 'begin_time', 'end_time', 'audio_duration', 'chapter_id', 'file', 'id'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Batched function to transcribe a batch of examples\n",
    "def transcribe_batch(batch):\n",
    "    # Extract audio arrays and sampling rates\n",
    "    audio_arrays = batch[\"audio\"]\n",
    "    inputs = [a[\"array\"] for a in audio_arrays]\n",
    "    sentences = batch[\"sentence\"]\n",
    "    # Transcribe using the pipeline\n",
    "    results = pipe(inputs)\n",
    "\n",
    "    # Extract just the text\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "\n",
    "    # Return the transcriptions as a new column\n",
    "    return {\n",
    "        \"transcription\": transcriptions\n",
    "    }\n",
    "\n",
    "# Apply the batched function to the dataset\n",
    "transcriptions = dataset.map(transcribe_batch, batched=True, batch_size=8)  # You can tune batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "total_wer = 0\n",
    "total_cer = 0\n",
    "\n",
    "# Store the WER and CER values for averaging later\n",
    "wer_values = []\n",
    "cer_values = []\n",
    "\n",
    "# Now calculate WER and CER for each transcription and store them\n",
    "for transcription in transcriptions:\n",
    "    ground_truth = transcription['sentence']\n",
    "    predicted_text = transcription['transcription']\n",
    "    \n",
    "    # Calculate WER and CER for each transcription\n",
    "    word_error_rate = wer(ground_truth, predicted_text)  # Calculate WER\n",
    "    char_error_rate = cer(ground_truth, predicted_text)  # Calculate CER\n",
    "    \n",
    "    # Append the values to lists for later averaging\n",
    "    wer_values.append(word_error_rate)\n",
    "    cer_values.append(char_error_rate)\n",
    "    \n",
    "    # Print the top 5 transcriptions based on WER (or you can choose CER as well)\n",
    "    if len(wer_values) <= 5:\n",
    "        print(f\"Audio Path: {transcription['audio_path']}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Transcription: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Calculate the average WER and CER\n",
    "avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0\n",
    "avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0\n",
    "\n",
    "# Print the average WER and CER\n",
    "print(\"\\nAverage WER:\", avg_wer)\n",
    "print(\"Average CER:\", avg_cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "fleurs = load_dataset(\"google/fleurs\", \"fr_fr\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fleurs.remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "# Batched function to transcribe a batch of examples\n",
    "def transcribe_batch(batch):\n",
    "    # Extract audio arrays and sampling rates\n",
    "    audio_arrays = batch[\"audio\"]\n",
    "    inputs = [a[\"array\"] for a in audio_arrays]\n",
    "    sentences = batch[\"sentence\"]\n",
    "    # Transcribe using the pipeline\n",
    "    results = pipe(inputs)\n",
    "\n",
    "    # Extract just the text\n",
    "    transcriptions = [r[\"text\"] for r in results]\n",
    "\n",
    "    # Return the transcriptions as a new column\n",
    "    return {\n",
    "        \"transcription\": transcriptions\n",
    "    }\n",
    "\n",
    "# Apply the batched function to the dataset\n",
    "transcriptions = dataset.map(transcribe_batch, batched=True, batch_size=8)  # You can tune batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "total_wer = 0\n",
    "total_cer = 0\n",
    "\n",
    "# Store the WER and CER values for averaging later\n",
    "wer_values = []\n",
    "cer_values = []\n",
    "\n",
    "# Now calculate WER and CER for each transcription and store them\n",
    "for transcription in transcriptions:\n",
    "    ground_truth = transcription['sentence']\n",
    "    predicted_text = transcription['transcription']\n",
    "    \n",
    "    # Calculate WER and CER for each transcription\n",
    "    word_error_rate = wer(ground_truth, predicted_text)  # Calculate WER\n",
    "    char_error_rate = cer(ground_truth, predicted_text)  # Calculate CER\n",
    "    \n",
    "    # Append the values to lists for later averaging\n",
    "    wer_values.append(word_error_rate)\n",
    "    cer_values.append(char_error_rate)\n",
    "    \n",
    "    # Print the top 5 transcriptions based on WER (or you can choose CER as well)\n",
    "    if len(wer_values) <= 5:\n",
    "        print(f\"Audio Path: {transcription['audio_path']}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Transcription: {predicted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Calculate the average WER and CER\n",
    "avg_wer = sum(wer_values) / len(wer_values) if wer_values else 0\n",
    "avg_cer = sum(cer_values) / len(cer_values) if cer_values else 0\n",
    "\n",
    "# Print the average WER and CER\n",
    "print(\"\\nAverage WER:\", avg_wer)\n",
    "print(\"Average CER:\", avg_cer)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8185261,
     "sourceId": 12935016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8185275,
     "sourceId": 12935036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8192341,
     "sourceId": 12945680,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 441073,
     "modelInstanceId": 423552,
     "sourceId": 556973,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
